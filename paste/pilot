#!/usr/bin/env jruby
%W(optparse fileutils csv json #{ENV['HADOOP_BASE']}/libexec/lib/text-1.0-SNAPSHOT.jar).each { |e| require e }

class Analyzer
  def self.require_jars
    %w(
      com.google.guava:guava:14.0.1
      commons-logging:commons-logging:1.1.1
      log4j:log4j:1.2.17
      org.apache.avro:avro:1.5.3
      org.apache.commons:commons-math3:3.2
      org.apache.hadoop:hadoop-common:2.0.5-alpha
      org.apache.lucene:lucene-analyzers-common:4.3.0
      org.apache.lucene:lucene-core:4.3.0
      org.apache.mahout:mahout-core:0.8
      org.apache.mahout:mahout-math:0.8
      org.slf4j:slf4j-api:1.6.1
      org.slf4j:slf4j-log4j12:1.6.1
    ).map do |e|
      g, a, v = e.split(':')
      jar = "#{ENV['HOME']}/.m2/repository/#{g.gsub(/\./, '/')}/#{a}/#{v}/#{a}-#{v}.jar"
      system "mvn dependency:get -DremoteRepositories=http://download.java.net/maven2 -Dartifact=#{e}" unless File.exist?(jar)
      require jar
    end
  end

  def initialize(model_id, debug = false)
    @@jars ||= TopicModeling.require_jars
    a, b = "http://s3.amazonaws.com/#{ENV['S3_BUCKET']}/#{model_id}/{dictionary.file-0,model-0,df-count-0}", "/tmp/#{model_id}"
    system "curl -o '#{b}/#1' -ksL '#{a}' --create-dirs" unless Dir.exist?(b)
    $stderr.puts "DEBUG: openning model files (id: #{model_id})" if debug
    org.apache.log4j.Logger.root_logger.level = org.apache.log4j.Level::OFF
    dictionary, model, df = %w(dictionary.file-0 model-0 df-count-0).map { |e| open("#{b}/#{e}").to_inputstream }
    begin
      @modeling = com.henry4j.text.TopicModeling.new(dictionary, model, 'com.henry4j.text.CommTextAnalyzer', df)
    ensure
      [dictionary, model, df].each { |e| e and e.close }
    end
  end

  def p_topics(s, sort = false, min_value = nil)
    a = @modeling.get_ptopic(s).to_ary
    a = a.each_with_index.sort_by { |(e, i)| -e }.
        reduce({}) { |h, (e, i)| e *= 100; h[i] = e.round(1) unless min_value && e < min_value; h } if sort && !a[0].nan?
    a
  end
end

def tokenize(analzyer, text)
  stream = analzyer.tokenStream("{field-name}", java.io.StringReader.new(text)).tap { |s| s.reset }
  term_attr = stream.add_attribute(org.apache.lucene.analysis.tokenattributes.CharTermAttribute.java_class)
  tokens = []
  begin
    tokens << java.lang.String.new(term_attr.buffer, 0, term_attr.length).to_s if term_attr.length > 0 while stream.increment_token
    tokens
  ensure
    stream.close
  end
end

def parse_options
  options = {}
  OptionParser.new do |p|
    p.on('-s', '--skip-entries INTEGER', Integer, 'Skips processing as many entries as specified (default: 1).') { |v| $options[:skip_count] = v }
    p.on('-n', '--max-entries INTEGER', Integer, 'Processes as many entries as specified.') { |v| $options[:max_entries] = v }
    p.on('-i', '--id-field INTEGER', Integer, 'Specifies required field index for document id.') { |v| $options[:id_field] = v }
    p.on('-l', '--l-field INTEGER', Integer, 'Specifies required field index for label(s).') { |v| $options[:l_field] = v }
    p.on('-f', '--fields i,j,k', Array, 'Specifies required field indices for feature(s).') { |v| $options[:fields] = v }
    p.on('-r', '--stop-phrases PATH', String, 'Specifies optional file paths for stop-phrases.') { |v| $options[:stop_phrases] = v }
    p.on('-o', '--overwrite', 'Specifies whether to overwrite existing output.') { |v| $options[:overwrite] = v }
    p.on('-x', '--excludes x,y,z', Array, 'Specifies labels to exclude.') { |v| $options[:excludes] = v }
  end.parse!
  options
end

def run!
  Analyzer.require_jars

  options = parse_options
  overwrite = options[:overwrite]
  skip_count = options[:skip_count] || 1
  max_entries = options[:max_entries]
  out_dir = options[:out_dir] || '/tmp'
  id_field = options[:id_field] || 0
  l_field = options[:l_field] || 8
  fields = (options[:fields] || ['4', '5']).map { |e| e.to_i }
  stop_phrases = options[:stop_phrases] || File.expand_path('../stop-comm-text', File.realpath(__FILE__))
  excludes = options[:excludes] || []

  # fail "'corpus' must be specified." unless (corpus = ARGV[0])
  corpus = File.join(ENV['MAHOUT_WORK'], 'corpus-3492-876.csv')

  stop_phrases = File.exists?(stop_phrases) ? open(stop_phrases).readlines.reduce([]) { |a, l| l.start_with?('#') ? a : a << l.chomp } : nil
  stop_phrases = /#{stop_phrases.join('|')}/ if stop_phrases

  FileUtils.mkdir_p File.join(out_dir, 'corpus')

  w = CSV.open('/tmp/aaa.csv', 'w:windows-1250')

  lines_by_id = {}
  open(corpus, 'r:windows-1250') do |io|
    CSV.new(io).each do |l|
      next if (skip_count -= 1) >= 0
      next if l_field && l[l_field].nil?
      next if excludes.any? { |e| l[l_field].include?(e) }
      break if max_entries && (max_entries -= 1) < 0
      lines_by_id[l[id_field].strip] = l
    end
  end

  analyzer = org.apache.mahout.common.lucene.AnalyzerUtils.createAnalyzer('com.henry4j.text.CommTextAnalyzer')

  lines_by_id.sort.each do |id, l|
    begin
      s = fields.map { |f| l[f] }.compact.each { |f| stop_phrases ? f.gsub(stop_phrases, '.') : f }.join(' ')
      s = tokenize(analyzer, s).join(' ')
      w.puts [l[l_field], id, s]
    rescue
      raise 'Failed to process a line: %s.' % [l]
    end
    print id, ' '
  end

  puts 'hhhhello'
  exit 0
end

run! if __FILE__==$0
