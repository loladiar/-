#!/usr/bin/env jruby
%W(optparse open-uri fileutils csv json #{ENV['HADOOP_BASE']}/libexec/lib/text-1.0-SNAPSHOT.jar).each { |e| require e }

class Analyzer
  def self.require_jars
    %w(
      org.apache.lucene:lucene-analyzers-common:4.3.0
      org.apache.lucene:lucene-core:4.3.0
      org.apache.mahout:mahout-core:0.8
    ).map do |e|
      g, a, v = e.split(':')
      jar = "#{ENV['HOME']}/.m2/repository/#{g.gsub(/\./, '/')}/#{a}/#{v}/#{a}-#{v}.jar"
      system "mvn dependency:get -DremoteRepositories=http://download.java.net/maven2 -Dartifact=#{e}" unless File.exist?(jar)
      require jar
    end
  end

  def initialize(analyzer)
    @@jars ||= Analyzer.require_jars
    @analyzer = org.apache.mahout.common.lucene.AnalyzerUtils.createAnalyzer(analyzer)
  end

  def tokenize(text)
    stream = @analyzer.tokenStream('{field-name}', java.io.StringReader.new(text)).tap { |e| e.reset }
    term_attr = stream.add_attribute(org.apache.lucene.analysis.tokenattributes.CharTermAttribute.java_class)
    tokens = []
    begin
      tokens << java.lang.String.new(term_attr.buffer, 0, term_attr.length).to_s if term_attr.length > 0 while stream.increment_token
      tokens
    ensure
      stream.close
    end
  end
end

def parse_options
  options = {}
  OptionParser.new do |p|
    p.on('-s', '--skip-entries INTEGER', Integer, 'Skips processing as many entries as specified (default: 1).') { |v| options[:skip_count] = v }
    p.on('-n', '--max-entries INTEGER', Integer, 'Processes as many entries as specified.') { |v| options[:max_entries] = v }
    p.on('-i', '--id-field INTEGER', Integer, 'Specifies required field index for document id.') { |v| options[:id_field] = v }
    p.on('-l', '--l-field INTEGER', Integer, 'Specifies required field index for label(s).') { |v| options[:l_field] = v }
    p.on('-f', '--fields i,j,k', Array, 'Specifies required field indices for feature(s).') { |v| options[:fields] = v }
    p.on('-r', '--stop-phrases PATH', String, 'Specifies optional file paths for stop-phrases.') { |v| options[:stop_phrases] = v }
    p.on('-x', '--excludes x,y,z', Array, 'Specifies labels to exclude.') { |v| options[:excludes] = v }
  end.parse!
  options
end

def run!
  analyzer = Analyzer.new('com.henry4j.text.CommTextAnalyzer')
  options = parse_options
  skip_count = options[:skip_count] || 1
  max_entries = options[:max_entries]
  id_field = options[:id_field] || 0
  l_field = options[:l_field] || 3
  fields = (options[:fields] || ['1', '2']).map { |e| e.to_i }
  stop_phrases = options[:stop_phrases] || File.expand_path('../stop-comm-text', File.realpath(__FILE__))
  excludes = options[:excludes] || []

  stop_phrases = File.exists?(stop_phrases) ? open(stop_phrases).readlines.reduce([]) { |a, l| l.start_with?('#') ? a : a << l.chomp } : nil
  stop_phrases = /#{stop_phrases.join('|')}/ if stop_phrases

  lines_by_id = {}
  argf = ARGV.empty? ? [$stdin] : ARGV.map { |e| open(e, 'r:windows-1250') }
  begin
    argf.each do |io|
      CSV.new(io).each do |l|
        next if (skip_count -= 1) >= 0
        next if l_field && l[l_field].nil?
        next if excludes.any? { |e| l[l_field].include?(e) }
        break if max_entries && (max_entries -= 1) < 0
        lines_by_id[-l[id_field].strip.to_i] = l
      end
    end
  ensure
    argf.each { |io| io.close } 
  end

  w = CSV.new($stdout)
  w.puts ['label', 'id', 'text']
  lines_by_id.sort.each do |id, l|
    begin
      s = fields.map { |f| l[f] }.compact.each { |f| stop_phrases ? f.gsub(stop_phrases, '.') : f }.join(' ')
      s = analyzer.tokenize(s).join(' ')
      w.puts [-id, l[l_field], s]
    rescue
      raise 'Failed to process a line: %s.' % [l]
    end
  end
  exit 0
end

run! if __FILE__==$0
