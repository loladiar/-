#!/usr/bin/env jruby
%w{optparse fileutils open-uri csv json}.each { |e| require e }

$options = {}
OptionParser.new do |p|
  p.on('-s', '--skip-entries INTEGER', Integer, 'Skips processing as many entries as specified (default: 1).') { |v| $options[:skip_count] = v }
  p.on('-n', '--max-entries INTEGER', Integer, 'Processes as many entries as specified.') { |v| $options[:max_entries] = v }
  p.on('-d', '--out-dir PATH', String, 'Specifies an optional output directory path (default: /tmp/).') { |v| $options[:out_dir] = v }
  p.on('-i', '--id-field INTEGER', Integer, 'Specifies required field index for document id.') { |v| $options[:id_field] = v }
  p.on('-l', '--l-field INTEGER', Integer, 'Specifies required field index for label(s).') { |v| $options[:l_field] = v }
  p.on('-f', '--fields i,j,k', Array, 'Specifies required field indices for feature(s).') { |v| $options[:fields] = v }
  p.on('-x', '--stop-phrases PATH', String, 'Specifies optional file paths for stop-phrases.') { |v| $options[:stop_phrases] = v }
  p.on('-o', '--overwrite', 'Whether to overwrite existing corpus and corpus-priors.') { |v| $options[:overwrite] = v }
end.parse!

fail "'corpus' must be specified." unless (corpus = ARGV[0])

overwrite = $options[:overwrite]
skip_count = $options[:skip_count] || 1
max_entries = $options[:max_entries]
out_dir = $options[:out_dir] || '/tmp'
id_field = $options[:id_field] || 4
l_field = $options[:l_field]
fields = ($options[:fields] || ['1', '3']).map { |e| e.to_i }
stop_phrases = $options[:stop_phrases] || File.expand_path('../stop-comm-text', File.realpath(__FILE__))

if l_field
  %w(
    com.google.guava:guava:14.0.1
    commons-configuration:commons-configuration:1.6
    commons-logging:commons-logging:1.1.1
    commons-lang:commons-lang:2.5
    log4j:log4j:1.2.17
    org.apache.avro:avro:1.5.3
    org.apache.commons:commons-math3:3.2
    org.apache.hadoop:hadoop-common:2.0.5-alpha
    org.apache.hadoop:hadoop-annotations:2.0.5-alpha
    org.apache.hadoop:hadoop-auth:2.0.5-alpha
    org.apache.lucene:lucene-analyzers-common:4.3.0
    org.apache.lucene:lucene-core:4.3.0
    org.apache.mahout:mahout-core:0.8
    org.apache.mahout:mahout-math:0.8
    org.slf4j:slf4j-api:1.6.1
    org.slf4j:slf4j-log4j12:1.6.1
  ).map do |e|
    g, a, v = e.split(':')
    jar = "#{ENV['HOME']}/.m2/repository/#{g.gsub(/\./, '/')}/#{a}/#{v}/#{a}-#{v}.jar"
    system "mvn dependency:get -DremoteRepositories=http://download.java.net/maven2 -Dartifact=#{e}" unless File.exist?(jar)
    require jar
  end
end

stop_phrases = File.exists?(stop_phrases) ? open(stop_phrases).readlines.reduce([]) { |a, l| l.start_with?('#') ? a : a << l.chomp } : nil
stop_phrases = /#{stop_phrases.join('|')}/ if stop_phrases

FileUtils.rm_rf File.join(out_dir, 'corpus') if overwrite
FileUtils.rm_rf File.join(out_dir, 'corpus-priors') if overwrite
FileUtils.mkdir_p File.join(out_dir, 'corpus')
 
lines_by_id = {}
open(corpus, 'r:windows-1250') do |io|
  CSV.new(io).each do |l|
    next if (skip_count -= 1) >= 0
    next if l_field && l[l_field].nil?
    break if max_entries && (max_entries -= 1) < 0
    lines_by_id[l[id_field].strip] = l
  end
end

label_ids = {}
labels = []
puts 'Writing documents: '
lines_by_id.sort.each do |id, l|
  path = File.join(out_dir, "corpus", id + ".txt")
  open(path, 'w:UTF-8') do |w| 
    begin
      fields.map { |f| l[f] }.compact.each { |f| w.puts stop_phrases ? f.gsub(stop_phrases, '.') : f }
    rescue
      raise 'Failed to process a line: %s.' % [l]
    end
    print id, ' '
  end
  labels << l[l_field].split('|').map { |e| e.strip }.map { |e| label_ids[e] ||= label_ids.size } if l_field
end
puts

open(File.join(out_dir, 'label_ids.json')) do |w|
  w.puts JSON[label_ids]
  puts 'INFO: done writing to .json'
end

if l_field
  rows, columns = labels.size, label_ids.size
  doc_topic_priors = org.apache.mahout.math.SparseRowMatrix.new(rows, columns, true) # true for random access
  labels.each_with_index { |e, i| e.each { |l| doc_topic_priors.view_row(i).set_quick(l, 1.0/e.size) } }
  path = org.apache.hadoop.fs.Path.new('file://' + File.absolute_path(File.join(out_dir, 'doc-topic-priors')))
  conf = org.apache.hadoop.conf.Configuration.new
  org.apache.mahout.math.MatrixUtils.write(path, conf, doc_topic_priors)
  puts 'INFO: done writing to %s (%d rows x %d columns)' % [path, rows, columns]
end
exit 0
