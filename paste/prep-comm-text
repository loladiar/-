#!/usr/bin/env jruby
%w{optparse fileutils open-uri csv json}.each { |e| require e }

$options = {}
OptionParser.new do |p|
  p.on('-s', '--skip-entries INTEGER', Integer, 'Skips processing as many entries as specified (default: 1).') { |v| $options[:skip_count] = v }
  p.on('-n', '--max-entries INTEGER', Integer, 'Processes as many entries as specified.') { |v| $options[:max_entries] = v }
  p.on('-d', '--out-dir PATH', String, 'Specifies an optional output directory path (default: /tmp/).') { |v| $options[:out_dir] = v }
  p.on('-i', '--id-field INTEGER', Integer, 'Specifies required field index for document id.') { |v| $options[:id_field] = v }
  p.on('-l', '--l-field INTEGER', Integer, 'Specifies required field index for label(s).') { |v| $options[:l_field] = v }
  p.on('-f', '--fields i,j,k', Array, 'Specifies required field indices for feature(s).') { |v| $options[:fields] = v }
  p.on('-x', '--stop-phrases PATH', String, 'Specifies optional file paths for stop-phrases.') { |v| $options[:stop_phrases] = v }
  p.on('-o', '--overwrite', 'Whether to overwrite existing corpus and corpus-priors.') { |v| $options[:overwrite] = v }
end.parse!

fail "'corpus' must be specified." unless (corpus = ARGV[0])

overwrite = $options[:overwrite]
skip_count = $options[:skip_count] || 1
max_entries = $options[:max_entries]
out_dir = $options[:out_dir] || '/tmp'
id_field = $options[:id_field] || 4
l_field = $options[:l_field]
fields = ($options[:fields] || ['1', '3']).map { |e| e.to_i }
stop_phrases = $options[:stop_phrases] || File.expand_path('../stop-comm-text', File.realpath(__FILE__))

if l_field
  %w(
    com.google.guava:guava:14.0.1
    commons-configuration:commons-configuration:1.6
    commons-logging:commons-logging:1.1.1
    commons-lang:commons-lang:2.5
    log4j:log4j:1.2.17
    org.apache.avro:avro:1.5.3
    org.apache.commons:commons-math3:3.2
    org.apache.hadoop:hadoop-common:2.0.5-alpha
    org.apache.hadoop:hadoop-annotations:2.0.5-alpha
    org.apache.hadoop:hadoop-auth:2.0.5-alpha
    org.apache.lucene:lucene-analyzers-common:4.3.0
    org.apache.lucene:lucene-core:4.3.0
    org.apache.mahout:mahout-core:0.8
    org.apache.mahout:mahout-math:0.8
    org.slf4j:slf4j-api:1.6.1
    org.slf4j:slf4j-log4j12:1.6.1
  ).map do |e|
    g, a, v = e.split(':')
    jar = "#{ENV['HOME']}/.m2/repository/#{g.gsub(/\./, '/')}/#{a}/#{v}/#{a}-#{v}.jar"
    system "mvn dependency:get -DremoteRepositories=http://download.java.net/maven2 -Dartifact=#{e}" unless File.exist?(jar)
    require jar
  end
end

stop_phrases = File.exists?(stop_phrases) ? open(stop_phrases).readlines.reduce([]) { |a, l| l.start_with?('#') ? a : a << l.chomp } : nil
stop_phrases = /#{stop_phrases.join('|')}/ if stop_phrases

label_ids = {}
labels_by_doc_id = {}

open(corpus, 'r:windows-1250') do |io|
  FileUtils.rm_rf File.join(out_dir, 'corpus') if overwrite
  FileUtils.rm_rf File.join(out_dir, 'corpus-priors') if overwrite
  FileUtils.mkdir_p File.join(out_dir, 'corpus')
  CSV.new(io).each do |l|
    next if (skip_count -= 1) >= 0
    next if l_field && (labels = l[l_field]).nil?
    break if max_entries && (max_entries -= 1) < 0
    path = File.join(out_dir, "corpus", (id = l[id_field].strip) + ".txt")
    unless File.exist?(path)
      open(path, 'w:UTF-8') do |w| 
        fields.each do |f| 
          begin
            w.puts stop_phrases ? l[f].gsub(stop_phrases, '.') : l[f] if l[f]
          rescue
            raise 'Failed to process a line: %s.' % [l]
          end
        end
      end
      puts "INFO: done writing to '#{path}'."
    end
    labels_by_doc_id[id] = labels.split('|').map { |e| e.strip }.map { |e| label_ids[e] ||= label_ids.size } if l_field
  end
end
if l_field
  priors = org.apache.mahout.math.SparseRowMatrix.new(labels_by_doc_id.size, label_ids.size, true) # true for random access
  labels_by_doc_id.sort.each_with_index { |(k, v), i| v.each { |l| priors.view_row(i).set_quick(l, 1.0/v.size) } }
  path = org.apache.hadoop.fs.Path.new('file:///' + File.absolute_path(File.join(out_dir, 'corpus-priors')))
  conf = org.apache.hadoop.conf.Configuration.new
  org.apache.mahout.math.MatrixUtils.write(path, conf, priors)
  puts 'INFO: done writing to %s (%d rows x %d columns)' % [path, priors.row_size, priors.column_size]
end
exit 0
