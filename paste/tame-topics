#!/bin/bash # called `tame-corpus-basic`
if [ -z $4 ]; then ANALYZER='com.henry4j.text.CommTextAnalyzer'; else ANALYZER=$4; fi
if [ -z $3 ]; then MIN_LLR='120'; else export MIN_LLR=$3; fi
if [ -z $2 ]; then MAX_NGRAM='1'; else export MAX_NGRAM=$2; fi
if [ -z $1 ]; then WORK_ID='true-l-lda'; else export WORK_ID=$1; fi

export HADOOP_CLASSPATH=${MAHOUT_BASE}/lib/text-1.0-SNAPSHOT.jar:${MAHOUT_BASE}/lib/lucene-analyzers-common-4.3.0.jar:${MAHOUT_BASE}/lib/lucene-core-4.3.0.jar

if ! $($HADOOP dfs -test -e "${MAHOUT_WORK}/${WORK_ID}/matrix"); then
  $MAHOUT seq2sparse \
    -i ${MAHOUT_WORK}/comm-text-seq/ -o ${MAHOUT_WORK}/${WORK_ID} -ow --namedVector \
    -s 80 -md 40 -x 65 \
    -ng $MAX_NGRAM -ml $MIN_LLR \
    -a $ANALYZER
  
    # excludes terms of 80- DF & 70+ DF%
    # -a com.henry4j.text.CommTextAnalyzer \
    # -a org.apache.mahout.text.MailArchivesClusteringAnalyzer \
    # -a org.apache.lucene.analysis.en.EnglishAnalyzer \
    # -a org.apache.lucene.analysis.standard.StandardAnalyzer \

  for e in matrix docIndex; do $HADOOP dfs -rm ${MAHOUT_WORK}/${WORK_ID}/$e; done
  $MAHOUT rowid -i ${MAHOUT_WORK}/${WORK_ID}/tfidf-vectors -o ${MAHOUT_WORK}/${WORK_ID}
  for e in df-count tokenized-documents tfidf-vectors; do resplit ${MAHOUT_WORK}/${WORK_ID}/$e; done
  $MAHOUT seqdumper -i ${MAHOUT_WORK}/${WORK_ID}/tokenized-documents-0 -o /tmp/${WORK_ID}-tokenized-docs.txt
  $HADOOP dfs -put /tmp/${WORK_ID}-tokenized-docs.txt ${MAHOUT_WORK}/${WORK_ID}
fi

$HADOOP dfs -test -e "${MAHOUT_WORK}/comm-text-ext/corpus-priors" && CVB_OPTS="-k 14 -pidt -dtp ${MAHOUT_WORK}/comm-text-ext/corpus-priors -cd 6e-10" || CVB_OPTS='-k 20 -cd 6e-4'
rm -rf ${MAHOUT_WORK}/${WORK_ID}-*
$HADOOP dfs -rmr ${MAHOUT_WORK}/${WORK_ID}/model
$HADOOP dfs -rmr ${MAHOUT_WORK}/${WORK_ID}/topics
$HADOOP dfs -rmr ${MAHOUT_WORK}/${WORK_ID}-modeling
mahout cvb \
  -dict ${MAHOUT_WORK}/${WORK_ID}/dictionary.file-0 \
  -i  ${MAHOUT_WORK}/${WORK_ID}/matrix \
  -o  ${MAHOUT_WORK}/${WORK_ID}/model -ow \
  -mt ${MAHOUT_WORK}/${WORK_ID}-modeling \
  -dt ${MAHOUT_WORK}/${WORK_ID}/topics \
  -x 35 -block 2 -tf 0.25 -seed 777 \
  $CVB_OPTS

for e in model topics; do resplit ${MAHOUT_WORK}/${WORK_ID}/$e; done
for e in matrix docIndex wordcount frequency.file-0 tf-vectors; do $HADOOP dfs -rmr ${MAHOUT_WORK}/${WORK_ID}/$e; done

$MAHOUT vectordump \
  -i ${MAHOUT_WORK}/${WORK_ID}/model-0 -o /tmp/${WORK_ID}-w,z-dump.txt \
  -p true -sort ${MAHOUT_WORK}/${WORK_ID}/model-0 -vs 25 \
  -d ${MAHOUT_WORK}/${WORK_ID}/dictionary.file-0 -dt sequencefile
$MAHOUT vectordump -i ${MAHOUT_WORK}/${WORK_ID}/topics-0 -o /tmp/${WORK_ID}-z,d-dump.txt
$HADOOP dfs -put /tmp/${WORK_ID}-?,?-dump.txt ${MAHOUT_WORK}/${WORK_ID}

pp-w,z /tmp/${WORK_ID}-w,z-dump.txt | tee /tmp/${WORK_ID}-w,z-topic-terms.txt
pp-z,d /tmp/${WORK_ID}-z,d-dump.txt -n 10 | tee /tmp/${WORK_ID}-z,d-doc-topics.txt
for e in w,z z,d; do $HADOOP dfs -put /tmp/${WORK_ID}-$e-*.txt ${MAHOUT_WORK}/${WORK_ID}; done

rm -rf ${MAHOUT_WORK}/${WORK_ID}
$HADOOP dfs -get ${MAHOUT_WORK}/${WORK_ID} ${MAHOUT_WORK}/${WORK_ID}
s3cmd put -r ${MAHOUT_WORK}/${WORK_ID} s3://${S3_BUCKET}
s3cmd setacl -r --acl-public s3://${S3_BUCKET}/${WORK_ID}
