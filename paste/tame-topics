#!/usr/bin/env jruby # called `tame-topics`
%w{rake optparse open-uri csv json open3}.each { |e| require e }

$options = {}
OptionParser.new do |p|
  p.on('-a', '--analyzer STRING', String, "Specifies the analyzer (default: 'com.henry4j.text.CommTextAnalyzer')") { |v| $options[:analyzer] = v }
  p.on('-k', '--min-llr INTEGER', Integer, 'Specifies the min-LLR (default: 120)') { |v| $options[:min_llr] = v }
  p.on('-g', '--max-ngram INTEGER', Integer, 'Specifies the max N gram (default: 1)') { |v| $options[:max_ngram] = v }
  p.on('-w', '--work-id STRING', String, "Specifies the topic modeling work id (default: 'true-l-lda')") { |v| $options[:work_id] = v }
  p.on('-o', '--overwrite', 'Whether to overwrite existing doc-topics and model.') { |v| $options[:overwrite] = v }
end.parse!

def x!(*cmd, &blk) block_given? ? (sh cmd.join(' ') do |*a| blk.call(a) end) : (sh cmd.join(' ')) end

def store_topic_term_priors(work_id, vectors)
  Vectors.write vectors, "#{ENV['MAHOUT_WORK']}/#{work_id}/topic-term-priors"
end

def doc_topic_priors_exist?
  %x($HADOOP dfs -ls ${MAHOUT_WORK}/comm-text-ext/doc-topic-priors) && 0 == $?
end

def load_doc_topic_priors(work_id)
  Vectors.read "#{ENV['MAHOUT_WORK']}/comm-text-ext/doc-topic-priors"
end

def load_doc_vectors(work_id)
  Vectors.read "#{ENV['MAHOUT_WORK']}/#{work_id}/matrix"
end

def patch_mahout
  %w(core-0.8 core-0.8-job examples-0.8 examples-0.8-job)
    .map { |e| File.join(ENV['MAHOUT_BASE'], "mahout-%s.jar" % e) }
    .each { |e| FileUtils.mv(e, "#{File.dirname(e)}/#{File.basename(e)}.bak") if File.exist?(e) }
  %w(core-0.8.2 core-0.8.2-job examples-0.8.2 examples-0.8.2-job)
    .map { |e| File.join(ENV['MAHOUT_BASE'], "mahout-%s.jar" % e) }
    .reject { |e| File.exist?(e) }
    .each { |e| x! 'curl -o %s -kL http://dl.dropbox.com/u/47820156/mahout/l-lda/%s' % [e, File.basename(e)] }
end

analyzer = $options[:analyzer] || 'com.henry4j.text.CommTextAnalyzer'
min_llr = $options[:min_llr] || 120
max_ngram = $options[:max_ngram] || 1
work_id = $options[:work_id] || 'lda-' + Time.now.utc.strftime('%Y%m%d-%Hz')
vectors = $options[:vectors] || 'tf-vectors'
x! "$HADOOP dfs -rmr '${MAHOUT_WORK}/#{work_id}'" do end if $options[:overwrite]

if %x($HADOOP dfs -test -e "${MAHOUT_WORK}/#{work_id}/matrix") && 0 != $?.exitstatus
  x! [
    'export HADOOP_CLASSPATH=${MAHOUT_BASE}/lib/text-1.0-SNAPSHOT.jar:${MAHOUT_BASE}/lib/lucene-analyzers-common-4.3.0.jar:${MAHOUT_BASE}/lib/lucene-core-4.3.0.jar',
    "$MAHOUT seq2sparse -i ${MAHOUT_WORK}/comm-text-seq/ -o ${MAHOUT_WORK}/#{work_id} -ow --namedVector -s 20 -md 10 -x 65 -ng %s -ml %s -a %s" % [max_ngram, min_llr, analyzer]
  ].join('; ') # excludes terms of 10- DF & 65+ DF%
  # -a org.apache.mahout.text.MailArchivesClusteringAnalyzer \
  # -a org.apache.lucene.analysis.en.EnglishAnalyzer \
  # -a org.apache.lucene.analysis.standard.StandardAnalyzer \

  x! "$MAHOUT rowid -i ${MAHOUT_WORK}/#{work_id}/#{vectors} -o ${MAHOUT_WORK}/#{work_id}"
  %w(df-count tf-vectors tfidf-vectors tokenized-documents).each { |e| x! "resplit ${MAHOUT_WORK}/#{work_id}/#{e}" do end }
  x! "$MAHOUT seqdumper -i ${MAHOUT_WORK}/#{work_id}/tokenized-documents-0 -o /tmp/#{work_id}-tokenized-documents.txt"
  x! "$HADOOP dfs -put /tmp/#{work_id}-tokenized-documents.txt ${MAHOUT_WORK}/#{work_id}/tokenized-documents.txt"
end

io = %w(matrix dictionary.file-0 model doc-topics modeling).map { |e| "$MAHOUT_WORK/#{work_id}/#{e}" }
x! "$HADOOP dfs -rmr #{io[-3..-1].join(' ')}" do end

if doc_topic_priors_exist? 
  require_relative 'vectors'
  doc_topic_priors = load_doc_topic_priors(work_id)
  doc_vectors = load_doc_vectors(work_id)
  rows, columns = doc_topic_priors[0].size, doc_vectors[0].size
  topic_term_priors = org.apache.mahout.math.SparseRowMatrix.new(rows, columns, true) # true for random access
  doc_vectors.each do |(d, v)|
    doc_topic_priors[d].non_zeroes.each do |z_d|
      row = topic_term_priors.view_row(z_d.index)
      v.non_zeroes.each { |w| row.set_quick(w.index, row.get_quick(w.index) + w.get * z_d.get) }
    end
  end
  store_topic_term_priors(work_id, topic_term_priors)
  x! "$HADOOP dfs -cp ${MAHOUT_WORK}/comm-text-ext/labels.json ${MAHOUT_WORK}/#{work_id}"
  x! "$HADOOP dfs -cp ${MAHOUT_WORK}/#{work_id}/topic-term-priors #{io[-1]}/model-0/part-r-00000" do end
  patch_mahout
  cvb_opts = "-k #{rows} -pidt -dtp ${MAHOUT_WORK}/comm-text-ext/doc-topic-priors -cd 6e-24"
else
  cvb_opts = '-k 20 -pidt -cd 6e-4'
end

x! "rm -rf ${MAHOUT_WORK}/#{work_id}/modeling" do end
x! "$MAHOUT cvb -i %s -dict %s -ow -o %s -dt %s -mt %s -x 35 -block 2 -tf 0.25 -seed 777 #{cvb_opts}" % io
x! "resplit #{io[2, 2].join(' ')}"

x! "$MAHOUT vectordump -i %s-0 -sort %s-0 -d %s -o /tmp/#{work_id}-w,z-dump.txt -p true -vs 25 -dt sequencefile" % io.values_at(-3, -3, 1)
x! "$MAHOUT vectordump -i %s-0 -o /tmp/#{work_id}-z,d-dump.txt" % io[-2]
x! "pp-w,z /tmp/#{work_id}-w,z-dump.txt       | tee /tmp/#{work_id}-w,z-topic-terms.txt"
x! "pp-z,d /tmp/#{work_id}-z,d-dump.txt -n 30 | tee /tmp/#{work_id}-z,d-doc-topics.txt"
%w(w,z z,d).map { |e| x! "$HADOOP dfs -put /tmp/#{work_id}-#{e}-*.txt ${MAHOUT_WORK}/#{work_id}" do end }
 
x! "rm -rf ${MAHOUT_WORK}/#{work_id}"
x! "$HADOOP dfs -get $MAHOUT_WORK/#{work_id} ${MAHOUT_WORK}/#{work_id}"
%w(df-count-0 dictionary.file-0 model-0 labels.json tf-vectors-0 tfidf-vectors-0 tokenized-documents.txt topic-0).each { |e| x! "s3cmd put $MAHOUT_WORK/#{work_id}/#{e} s3://${S3_BUCKET}/#{work_id}/" }
%w(w,z-dump.txt w,z-topic-terms.txt doc-topics.txt z,d-dump.txt).each { |e| x! "s3cmd put $MAHOUT_WORK/#{work_id}/#{work_id}-#{e} s3://${S3_BUCKET}/#{work_id}/" }
x! "s3cmd setacl -r --acl-public s3://${S3_BUCKET}/#{work_id}"

