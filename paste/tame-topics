#!/usr/bin/env jruby # called `tame-topics`
%w{rake optparse open-uri csv json open3}.each { |e| require e }

$options = {}
OptionParser.new do |p|
  p.on('-a', '--analyzer STRING', String, "Specifies the analyzer (default: 'com.henry4j.text.CommTextAnalyzer')") { |v| $options[:analyzer] = v }
  p.on('-k', '--min-llr INTEGER', Integer, 'Specifies the min-LLR (default: 120)') { |v| $options[:min_llr] = v }
  p.on('-g', '--max-ngram INTEGER', Integer, 'Specifies the max N gram (default: 1)') { |v| $options[:max_ngram] = v }
  p.on('-w', '--work-id STRING', String, "Specifies the topic modeling work id (default: 'true-l-lda')") { |v| $options[:work_id] = v }
end.parse!

analyzer = $options[:analyzer] || 'com.henry4j.text.CommTextAnalyzer'
min_llr = $options[:min_llr] || 120
max_ngram = $options[:max_ngram] || 1
work_id = $options[:work_id] || 'true-l-lda'

def x!(*cmd, &blk) block_given? ? (sh cmd.join(' ') do |*a| blk.call(a) end) : (sh cmd.join(' ')) end

if %x($HADOOP dfs -test -e "${MAHOUT_WORK}/#{work_id}/matrix") && 0 != $?.exitstatus
  x! [
    'export HADOOP_CLASSPATH=${MAHOUT_BASE}/lib/text-1.0-SNAPSHOT.jar:${MAHOUT_BASE}/lib/lucene-analyzers-common-4.3.0.jar:${MAHOUT_BASE}/lib/lucene-core-4.3.0.jar',
    "$MAHOUT seq2sparse -i ${MAHOUT_WORK}/comm-text-seq/ -o ${MAHOUT_WORK}/#{work_id} -ow --namedVector -s 80 -md 40 -x 65 -ng %s -ml %s -a %s" % [max_ngram, min_llr, analyzer]
  ].join('; ') # excludes terms of 80- DF & 70+ DF%
  # -a org.apache.mahout.text.MailArchivesClusteringAnalyzer \
  # -a org.apache.lucene.analysis.en.EnglishAnalyzer \
  # -a org.apache.lucene.analysis.standard.StandardAnalyzer \

  x! "$MAHOUT rowid -i ${MAHOUT_WORK}/#{work_id}/tfidf-vectors -o ${MAHOUT_WORK}/#{work_id}"
  %w(df-count tfidf-vectors tokenized-documents).each { |e| x! "resplit ${MAHOUT_WORK}/#{work_id}/#{e}" do end }
  x! "$MAHOUT seqdumper -i ${MAHOUT_WORK}/#{work_id}/tokenized-documents-0 -o /tmp/#{work_id}-tokenized-documents.txt"
  x! "$HADOOP dfs -put /tmp/#{work_id}-tokenized-documents.txt ${MAHOUT_WORK}/#{work_id}/tokenized-documents.txt"
end

if %x($HADOOP dfs -test -e "${MAHOUT_WORK}/comm-text-ext/corpus-priors") && 0 == $?.exitstatus
  cvb_opts = '-k 14 -pidt -dtp ${MAHOUT_WORK}/comm-text-ext/corpus-priors -cd 6e-10'
  %w(core-0.8 core-0.8-job examples-0.8 examples-0.8-job)
    .map { |e| File.join(ENV['MAHOUT_BASE'], "mahout-%s.jar" % e) }
    .each { |e| FileUtils.mv(e, "#{File.dirname(e)}/#{File.basename(e)}.bak") if File.exist?(e) }
  %w(core-0.8.2 core-0.8.2-job examples-0.8.2 examples-0.8.2-job)
    .map { |e| File.join(ENV['MAHOUT_BASE'], "mahout-%s.jar" % e) }
    .reject { |e| File.exist?(e) }
    .each { |e| x! 'curl -o %s -kL http://dl.dropbox.com/u/47820156/mahout/l-lda/%s' % [e, File.basename(e)] }
else
  cvb_opts ='-k 20 -cd 6e-4'
end

io = %w(matrix dictionary.file-0 model topics modeling).map { |e| "$MAHOUT_WORK/#{work_id}/#{e}" }
x! "$HADOOP dfs -rmr #{io[-3..-1].join(' ')}" do end
x! "rm -rf ${MAHOUT_WORK}/#{work_id}/modeling" do end
x! "$MAHOUT cvb -i %s -dict %s -ow -o %s -dt %s -mt %s -x 3 -block 2 -tf 0.25 -seed 777 #{cvb_opts}" % io
x! "resplit #{io[-2..-1].join(' ')}"

# $MAHOUT vectordump \
#   -i ${MAHOUT_WORK}/${WORK_ID}/model-0 -o /tmp/${WORK_ID}-w,z-dump.txt \
#   -p true -sort ${MAHOUT_WORK}/${WORK_ID}/model-0 -vs 25 \
#   -d ${MAHOUT_WORK}/${WORK_ID}/dictionary.file-0 -dt sequencefile
# $MAHOUT vectordump -i ${MAHOUT_WORK}/${WORK_ID}/topics-0 -o /tmp/${WORK_ID}-z,d-dump.txt
# $HADOOP dfs -put /tmp/${WORK_ID}-?,?-dump.txt ${MAHOUT_WORK}/${WORK_ID}
# 
# pp-w,z /tmp/${WORK_ID}-w,z-dump.txt | tee /tmp/${WORK_ID}-w,z-topic-terms.txt
# pp-z,d /tmp/${WORK_ID}-z,d-dump.txt -n 10 | tee /tmp/${WORK_ID}-z,d-doc-topics.txt
# for e in w,z z,d; do $HADOOP dfs -put /tmp/${WORK_ID}-$e-*.txt ${MAHOUT_WORK}/${WORK_ID}; done
# 
# rm -rf ${MAHOUT_WORK}/${WORK_ID}
# $HADOOP dfs -get ${MAHOUT_WORK}/${WORK_ID} ${MAHOUT_WORK}/${WORK_ID}
# s3cmd put -r ${MAHOUT_WORK}/${WORK_ID} s3://${S3_BUCKET}
# s3cmd setacl -r --acl-public s3://${S3_BUCKET}/${WORK_ID}
