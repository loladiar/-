#!/usr/bin/env jruby # called `tame-topics`
%w{rake optparse open-uri csv json open3}.each { |e| require e }

$options = {}
OptionParser.new do |p|
  p.on('-a', '--analyzer STRING', String, "Specifies the analyzer (default: 'com.henry4j.text.CommTextAnalyzer')") { |v| $options[:analyzer] = v }
  p.on('-k', '--min-llr INTEGER', Integer, 'Specifies the min-LLR (default: 120)') { |v| $options[:min_llr] = v }
  p.on('-g', '--max-ngram INTEGER', Integer, 'Specifies the max N gram (default: 1)') { |v| $options[:max_ngram] = v }
  p.on('-w', '--work-id STRING', String, "Specifies the topic modeling work id (default: 'true-l-lda')") { |v| $options[:work_id] = v }
  p.on('-o', '--overwrite', 'Whether to overwrite existing corpus and corpus-priors.') { |v| $options[:overwrite] = v }
end.parse!

analyzer = $options[:analyzer] || 'com.henry4j.text.CommTextAnalyzer'
min_llr = $options[:min_llr] || 120
max_ngram = $options[:max_ngram] || 1
work_id = $options[:work_id] || 'true-l-lda'
x! "$HADOOP dfs -rmr '${MAHOUT_WORK}/#{work_id}'" do end if $options[:overwrite]
vectors = $options[:vectors] || 'tf-vectors'

def x!(*cmd, &blk) block_given? ? (sh cmd.join(' ') do |*a| blk.call(a) end) : (sh cmd.join(' ')) end

def require_jars
  %w(
    com.google.guava:guava:14.0.1
    commons-configuration:commons-configuration:1.6
    commons-logging:commons-logging:1.1.1
    commons-lang:commons-lang:2.5
    log4j:log4j:1.2.17
    org.apache.avro:avro:1.5.3
    org.apache.commons:commons-math3:3.2
    org.apache.hadoop:hadoop-common:2.0.5-alpha
    org.apache.hadoop:hadoop-annotations:2.0.5-alpha
    org.apache.hadoop:hadoop-auth:2.0.5-alpha
    org.apache.lucene:lucene-analyzers-common:4.3.0
    org.apache.lucene:lucene-core:4.3.0
    org.apache.mahout:mahout-core:0.8
    org.apache.mahout:mahout-math:0.8
    org.slf4j:slf4j-api:1.6.1
    org.slf4j:slf4j-log4j12:1.6.1
  ).map do |e|
    g, a, v = e.split(':')
    jar = "#{ENV['HOME']}/.m2/repository/#{g.gsub(/\./, '/')}/#{a}/#{v}/#{a}-#{v}.jar"
    system "mvn dependency:get -DremoteRepositories=http://download.java.net/maven2 -Dartifact=#{e}" unless File.exist?(jar)
    require jar
  end
end

def write_vectors(path, vectors, conf = org.apache.hadoop.conf.Configuration.new)
  path = org.apache.hadoop.fs.Path.new(path) if path.is_a? String
  org.apache.mahout.math.MatrixUtils.write(path, conf, vectors)
end

def store_topic_term_priors(work_id, vectors)
  a, b = "/tmp/#{work_id}-topic-term-priors", "${MAHOUT_WORK}/#{work_id}/topic-term-priors"
  write_vectors(a, vectors)
  x! "$HADOOP dfs -put #{a} #{b}" do end
end

def read_vectors(path, conf = org.apache.hadoop.conf.Configuration.new)
  path = org.apache.hadoop.fs.Path.new('file://' + path) if path.is_a? String
  fs = org.apache.hadoop.fs.FileSystem.get(conf)
  reader = org.apache.hadoop.io.SequenceFile::Reader.new(fs, path, conf)
  key, value = reader.key_class.new_instance, reader.value_class.new_instance
  vectors = {}
  vectors[key.get] = value.get while reader.next(key, value) 
  vectors
end

def read_df_vectors(path, conf = org.apache.hadoop.conf.Configuration.new)
  h = read_vectors(path, conf)
  a = h.reduce([]) { |a, (k, v)| a[k] = v unless k < 0 }
  a << h[-1]
end

def load_df_vectors(work_id)
  a, b = "${MAHOUT_WORK}/#{work_id}/df-count-0", "/tmp/#{work_id}-df-count-0"
  x! "$HADOOP dfs -get #{a} #{b}" do end
  read_vectors b
end

def load_doc_topic_priors(work_id)
  a, b = '${MAHOUT_WORK}/comm-text-ext/doc-topic-priors', "/tmp/#{work_id}-doc-topic-priors"
  if %x($HADOOP dfs -ls #{a}) && 0 == $?
    x! "$HADOOP dfs -get #{a} #{b}" do end unless File.exist?(b)
    require_jars
    read_vectors b
  end
end

def load_doc_vectors(work_id)
  a, b = "${MAHOUT_WORK}/#{work_id}/matrix", "/tmp/#{work_id}-matrix"
  x! "$HADOOP dfs -get #{a} #{b}" do end unless File.exist?(b)
  read_vectors b
end

def patch_mahout
  %w(core-0.8 core-0.8-job examples-0.8 examples-0.8-job)
    .map { |e| File.join(ENV['MAHOUT_BASE'], "mahout-%s.jar" % e) }
    .each { |e| FileUtils.mv(e, "#{File.dirname(e)}/#{File.basename(e)}.bak") if File.exist?(e) }
  %w(core-0.8.2 core-0.8.2-job examples-0.8.2 examples-0.8.2-job)
    .map { |e| File.join(ENV['MAHOUT_BASE'], "mahout-%s.jar" % e) }
    .reject { |e| File.exist?(e) }
    .each { |e| x! 'curl -o %s -kL http://dl.dropbox.com/u/47820156/mahout/l-lda/%s' % [e, File.basename(e)] }
end

if %x($HADOOP dfs -test -e "${MAHOUT_WORK}/#{work_id}/matrix") && 0 != $?.exitstatus
  x! [
    'export HADOOP_CLASSPATH=${MAHOUT_BASE}/lib/text-1.0-SNAPSHOT.jar:${MAHOUT_BASE}/lib/lucene-analyzers-common-4.3.0.jar:${MAHOUT_BASE}/lib/lucene-core-4.3.0.jar',
    "$MAHOUT seq2sparse -i ${MAHOUT_WORK}/comm-text-seq/ -o ${MAHOUT_WORK}/#{work_id} -ow --namedVector -s 20 -md 10 -x 65 -ng %s -ml %s -a %s" % [max_ngram, min_llr, analyzer]
  ].join('; ') # excludes terms of 80- DF & 70+ DF%
  # -a org.apache.mahout.text.MailArchivesClusteringAnalyzer \
  # -a org.apache.lucene.analysis.en.EnglishAnalyzer \
  # -a org.apache.lucene.analysis.standard.StandardAnalyzer \

  x! "$MAHOUT rowid -i ${MAHOUT_WORK}/#{work_id}/#{vectors} -o ${MAHOUT_WORK}/#{work_id}"
  %w(df-count tf-vectors tfidf-vectors tokenized-documents).each { |e| x! "resplit ${MAHOUT_WORK}/#{work_id}/#{e}" do end }
  x! "$MAHOUT seqdumper -i ${MAHOUT_WORK}/#{work_id}/tokenized-documents-0 -o /tmp/#{work_id}-tokenized-documents.txt"
  x! "$HADOOP dfs -put /tmp/#{work_id}-tokenized-documents.txt ${MAHOUT_WORK}/#{work_id}/tokenized-documents.txt"
end

if doc_topic_priors = load_doc_topic_priors(work_id)
  doc_vectors = load_doc_vectors(work_id)
  rows, columns = doc_topic_priors[0].size, doc_vectors[0].size
  topic_term_priors = org.apache.mahout.math.SparseRowMatrix.new(rows, columns, true) # true for random access
  doc_vectors.each do |(d, v)|
    doc_topic_priors[d].non_zeroes.each do |z_d|
      row = topic_term_priors.view_row(z_d.index)
      v.non_zeroes.each { |w| row.set_quick(w.index, row.get_quick(w.index) + w.get * z_d.get) }
      # p row.to_s
    end
  end
  store_topic_term_priors(work_id, topic_term_priors)
  patch_mahout
  cvb_opts = '-k 14 -pidt -dtp ${MAHOUT_WORK}/comm-text-ext/doc-topic-priors -cd 6e-10'
else
  cvb_opts ='-k 20 -pidt -cd 6e-4'
end

io = %w(matrix dictionary.file-0 model topics modeling).map { |e| "$MAHOUT_WORK/#{work_id}/#{e}" }
if %x($HADOOP dfs -test -e "${MAHOUT_WORK}/#{work_id}/model-0") && 0 != $?.exitstatus
  x! "$HADOOP dfs -rmr #{io[-3..-1].join(' ')}" do end
  x! "rm -rf ${MAHOUT_WORK}/#{work_id}/modeling" do end
  x! "$MAHOUT cvb -i %s -dict %s -ow -o %s -dt %s -mt %s -x 3 -block 2 -tf 0.25 -seed 777 #{cvb_opts}" % io
  x! "resplit #{io[2, 2].join(' ')}"
end  
exit -1

x! "$MAHOUT vectordump -i %s-0 -sort %s-0 -d %s -o /tmp/#{work_id}-w,z-dump.txt -p true -vs 25 -dt sequencefile" % io.values_at(-3, -3, 1)
x! "$MAHOUT vectordump -i %s-0 -o /tmp/#{work_id}-z,d-dump.txt" % io[-2]
x! "$HADOOP dfs -put /tmp/#{work_id}-*-dump.txt ${MAHOUT_WORK}/#{work_id}" do end
 
x! "pp-w,z /tmp/#{work_id}-w,z-dump.txt       | tee /tmp/#{work_id}-w,z-topic-terms.txt"
x! "pp-z,d /tmp/#{work_id}-z,d-dump.txt -n 10 | tee /tmp/#{work_id}-z,d-doc-topics.txt"
%w(w,z z,d).map { |e| x! "$HADOOP dfs -put /tmp/#{work_id}-#{e}-*.txt ${MAHOUT_WORK}/#{work_id}" do end }
 
x! "rm -rf ${MAHOUT_WORK}/#{work_id}"
x! "$HADOOP dfs -get $MAHOUT_WORK/#{work_id} ${MAHOUT_WORK}/#{work_id}"
%w(df-count-0 dictionary.file-0 model-0 tf-vectors-0 tfidf-vectors-0 tokenized-documents.txt topic-0).each { |e| x! "s3cmd put $MAHOUT_WORK/#{work_id}/#{e} s3://${S3_BUCKET}/#{work_id}/" }
%w(w,z-dump.txt w,z-topic-terms.txt doc-topics.txt z,d-dump.txt).each { |e| x! "s3cmd put $MAHOUT_WORK/#{work_id}/#{work_id}-#{e} s3://${S3_BUCKET}/#{work_id}/" }
x! "s3cmd setacl -r --acl-public s3://${S3_BUCKET}/#{work_id}"

