#!/usr/bin/env jruby
%w{optparse fileutils open-uri csv json}.each { |e| require e }
%w(
  com.google.guava:guava:14.0.1
  commons-configuration:commons-configuration:1.6
  commons-logging:commons-logging:1.1.1
  commons-lang:commons-lang:2.5
  log4j:log4j:1.2.17
  org.apache.avro:avro:1.5.3
  org.apache.commons:commons-math3:3.2
  org.apache.hadoop:hadoop-common:2.0.5-alpha
  org.apache.hadoop:hadoop-annotations:2.0.5-alpha
  org.apache.hadoop:hadoop-auth:2.0.5-alpha
  org.apache.lucene:lucene-analyzers-common:4.3.0
  org.apache.lucene:lucene-core:4.3.0
  org.apache.mahout:mahout-core:0.8
  org.apache.mahout:mahout-math:0.8
  org.slf4j:slf4j-api:1.6.1
  org.slf4j:slf4j-log4j12:1.6.1
).map do |e|
  g, a, v = e.split(':')
  jar = "#{ENV['HOME']}/.m2/repository/#{g.gsub(/\./, '/')}/#{a}/#{v}/#{a}-#{v}.jar"
  system "mvn dependency:get -DremoteRepositories=http://download.java.net/maven2 -Dartifact=#{e}" unless File.exist?(jar)
  require jar
end

$options = {}
OptionParser.new do |p|
  p.on('-s', '--skip-entries INTEGER', Integer, 'Skips processing as many entries as specified (default: 1).') { |v| $options[:skip_count] = v }
  p.on('-n', '--max-entries INTEGER', Integer, 'Processes as many entries as specified.') { |v| $options[:max_entries] = v }
  p.on('-d', '--out-dir PATH', String, 'Specifies an optional output directory path (default: /tmp/).') { |v| $options[:out_dir] = v }
  p.on('-i', '--id-field INTEGER', Integer, 'Specifies required field index for document id.') { |v| $options[:id_field] = v }
  p.on('-l', '--l-field INTEGER', Integer, 'Specifies required field index for label(s).') { |v| $options[:l_field] = v }
  p.on('-f', '--fields i,j,k', Array, 'Specifies required field indices for feature(s).') { |v| $options[:fields] = v }
  p.on('-x', '--stop-phrases PATH', String, 'Specifies optional file paths for stop-phrases.') { |v| $options[:stop_phrases] = v }
end.parse!

max_entries = $options[:max_entries]
out_dir = $options[:out_dir] || '/tmp/'
id_field = $options[:id_field] || 4
l_field = $options[:l_field] # || 8
fields = ($options[:fields] || ['1', '3']).map { |e| e.to_i }
stop_phrases = $options[:stop_phrases] || File.expand_path('../stop-comm-text', File.realpath(__FILE__))
stop_phrases = File.exists?(stop_phrases) ? open(stop_phrases).readlines.reduce([]) { |a, l| l.start_with?('#') ? a : a << l.chomp } : nil
stop_phrases = /#{stop_phrases.join('|')}/ if stop_phrases
puts stop_phrases
label_ids = {}
corpus_topics = []

ARGV.each do |e|
  open(e, 'r:windows-1250') do |f|
    skip_count = $options[:skip_count] || 1
    CSV.new(f).each do |l|
      next if (skip_count -= 1) >= 0
      next if l_field && (labels = l[l_field]).nil?
      break if max_entries && (max_entries -= 1) < 0
      FileUtils.mkdir_p out_dir unless Dir.exists?(out_dir)
      path = File.join(out_dir, (id = l[id_field].strip) + ".txt")
      puts "WARN: will over-write '#{path}'!!!" if File.exists?(path)
      open(path, 'w:UTF-8') do |w| 
        fields.each do |f| 
          begin
            w.puts stop_phrases ? l[f].gsub(stop_phrases, '.') : l[f] if l[f]
          rescue
            raise 'Failed to process a line: %s.' % [l]
          end
        end
      end
      corpus_topics << labels.split('|').map { |e| e.strip }.map { |e| label_ids[e] ||= label_ids.size } if l_field
      puts "INFO: done writing to '#{path}'."
    end
  end
  
  if l_field
    conf = org.apache.hadoop.conf.Configuration.new
    corpus_priors = org.apache.mahout.math.DenseMatrix.new(corpus_topics.size, label_ids.size)
    corpus_topics.each_with_index { |e, i| e.each { |t| corpus_priors.viewRow(i).setQuick(t, 1.0/e.size) } }
    path = org.apache.hadoop.fs.Path.new("file:///#{File.join(File.dirname(e), File.basename(e, '.*') + '-priors')}")
    org.apache.mahout.math.MatrixUtils.write(path, conf, corpus_priors)
  end
end
exit 0
