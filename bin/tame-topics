#!/bin/bash # called `tame-corpus-basic`
if [ -z $4 ]; then export ANALYZER='com.henry4j.text.CommTextAnalyzer'; else export ANALYZER=$4; fi
if [ -z $3 ]; then export MIN_LLR='120'; else export MIN_LLR=$3; fi
if [ -z $2 ]; then export MAX_NGRAM='2'; else export MAX_NGRAM=$2; fi
if [ -z $1 ]; then export WORK_ID='bigram-k'; else export WORK_ID=$1; fi

export HADOOP_CLASSPATH=${MAHOUT_BASE}/lib/text-1.0-SNAPSHOT.jar:${MAHOUT_BASE}/lib/lucene-analyzers-common-4.3.0.jar:${MAHOUT_BASE}/lib/lucene-core-4.3.0.jar
$HADOOP dfs -rmr ${MAHOUT_WORK}/${WORK_ID}*
$MAHOUT seq2sparse \
  -i ${MAHOUT_WORK}/comm-text-seq/ -o ${MAHOUT_WORK}/${WORK_ID} -ow --namedVector \
  -s 80 -md 40 -x 65 \
  -ng $MAX_NGRAM -ml $MIN_LLR \
  -a $ANALYZER

  # excludes terms of 80- DF & 70+ DF%
  # -a com.henry4j.text.CommTextAnalyzer \
  # -a org.apache.mahout.text.MailArchivesClusteringAnalyzer \
  # -a org.apache.lucene.analysis.en.EnglishAnalyzer \
  # -a org.apache.lucene.analysis.standard.StandardAnalyzer \

for e in matrix docIndex; do $HADOOP dfs -rm ${MAHOUT_WORK}/${WORK_ID}/$e; done
$MAHOUT rowid -i ${MAHOUT_WORK}/${WORK_ID}/tfidf-vectors -o ${MAHOUT_WORK}/${WORK_ID}
for e in df-count tokenized-documents tfidf-vectors; do resplit ${MAHOUT_WORK}/${WORK_ID}/$e; done
$MAHOUT seqdumper -i ${MAHOUT_WORK}/${WORK_ID}/tokenized-documents-0 -o /tmp/${WORK_ID}-tokenized-docs.txt
$HADOOP dfs -put /tmp/${WORK_ID}-tokenized-docs.txt ${MAHOUT_WORK}/${WORK_ID}

rm -rf ${MAHOUT_WORK}/${WORK_ID}-*
$HADOOP dfs -rmr ${MAHOUT_WORK}/${WORK_ID}-modeling
$MAHOUT cvb \
  -dict ${MAHOUT_WORK}/${WORK_ID}/dictionary.file-0 \
  -i  ${MAHOUT_WORK}/${WORK_ID}/matrix \
  -o  ${MAHOUT_WORK}/${WORK_ID}/model -ow \
  -mt ${MAHOUT_WORK}/${WORK_ID}-modeling \
  -dt ${MAHOUT_WORK}/${WORK_ID}/topics \
  -k 20 -x 35 -cd 6e-4 -block 2 -tf 0.25 -seed 777
for e in model topics; do resplit ${MAHOUT_WORK}/${WORK_ID}/$e; done
for e in matrix docIndex wordcount frequency.file-0 tf-vectors; do $HADOOP dfs -rmr ${MAHOUT_WORK}/${WORK_ID}/$e; done

$MAHOUT vectordump \
  -i ${MAHOUT_WORK}/${WORK_ID}/model-0 -o /tmp/${WORK_ID}-w,z-dump.txt \
  -p true -sort ${MAHOUT_WORK}/${WORK_ID}/model-0 -vs 25 \
  -d ${MAHOUT_WORK}/${WORK_ID}/dictionary.file-0 -dt sequencefile
$MAHOUT vectordump -i ${MAHOUT_WORK}/${WORK_ID}/topics-0 -o /tmp/${WORK_ID}-z,d-dump.txt
$HADOOP dfs -put /tmp/${WORK_ID}-?,?-dump.txt ${MAHOUT_WORK}/${WORK_ID}

pp-w,z /tmp/${WORK_ID}-w,z-dump.txt | tee /tmp/${WORK_ID}-w,z-topic-terms.txt
pp-z,d /tmp/${WORK_ID}-z,d-dump.txt -n 10 | tee /tmp/${WORK_ID}-z,d-doc-topics.txt
for e in w,z z,d; do $HADOOP dfs -put /tmp/${WORK_ID}-$e-*.txt ${MAHOUT_WORK}/${WORK_ID}; done

rm -rf ${MAHOUT_WORK}/${WORK_ID}
$HADOOP dfs -get ${MAHOUT_WORK}/${WORK_ID} ${MAHOUT_WORK}/${WORK_ID}
s3cmd put -r ${MAHOUT_WORK}/${WORK_ID} s3://${S3_BUCKET}
s3cmd setacl -r --acl-public s3://${S3_BUCKET}/${WORK_ID}
